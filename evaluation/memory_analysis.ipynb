{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HuggingFace Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6221ec27747a8e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 18:43:40.893422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-24 18:43:43,929] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from lib.alpaca_data import AlpacaDataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup, default_data_collator\n",
    "import datasets\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "import deepspeed\n",
    "import gc, psutil, threading\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:44.403925081Z",
     "start_time": "2024-01-25T01:43:37.928068592Z"
    }
   },
   "id": "d00d027c9f0dd5a9",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-24 18:43:44,404] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2024-01-24 18:43:44,405] [INFO] [comm.py:616:init_distributed] cdb=None\n",
      "[2024-01-24 18:43:44,406] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    }
   ],
   "source": [
    "# Deepspeed\n",
    "# 1. mock up the launcher\n",
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\"\n",
    "\n",
    "# 2. fix local_rank arg if it wasn't passed, in HF Trainer case it is:\n",
    "# if args.local_rank == -1: \n",
    "#     args.local_rank = 0\n",
    "\n",
    "# 3. finally init deepspeed dist and set the default device\n",
    "deepspeed.init_distributed()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:44.412406234Z",
     "start_time": "2024-01-25T01:43:44.406343643Z"
    }
   },
   "id": "fe94e11ee71e8339",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Converting Bytes to Megabytes\n",
    "def b2mb(x):\n",
    "    return int(x / 2**20)\n",
    "\n",
    "class TorchTracemalloc:\n",
    "    def __enter__(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_max_memory_allocated()  # reset the peak gauge to zero\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        self.process = psutil.Process()\n",
    "\n",
    "        self.cpu_begin = self.cpu_mem_used()\n",
    "        self.peak_monitoring = True\n",
    "        peak_monitor_thread = threading.Thread(target=self.peak_monitor_func)\n",
    "        peak_monitor_thread.daemon = True\n",
    "        peak_monitor_thread.start()\n",
    "        return self\n",
    "\n",
    "    def cpu_mem_used(self):\n",
    "        \"\"\"get resident set size memory for the current process\"\"\"\n",
    "        return self.process.memory_info().rss\n",
    "\n",
    "    def peak_monitor_func(self):\n",
    "        self.cpu_peak = -1\n",
    "\n",
    "        while True:\n",
    "            self.cpu_peak = max(self.cpu_mem_used(), self.cpu_peak)\n",
    "\n",
    "            # can't sleep or will not catch the peak right (this comment is here on purpose)\n",
    "            # time.sleep(0.001) # 1msec\n",
    "\n",
    "            if not self.peak_monitoring:\n",
    "                break\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.peak_monitoring = False\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        self.end = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used = b2mb(self.end - self.begin)\n",
    "        self.peaked = b2mb(self.peak - self.begin)\n",
    "\n",
    "        self.cpu_end = self.cpu_mem_used()\n",
    "        self.cpu_used = b2mb(self.cpu_end - self.cpu_begin)\n",
    "        self.cpu_peaked = b2mb(self.cpu_peak - self.cpu_begin)\n",
    "        # print(f\"delta used/peak {self.used:4d}/{self.peaked:4d}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:44.519907468Z",
     "start_time": "2024-01-25T01:43:44.414459736Z"
    }
   },
   "id": "e8e71f26c9e27959",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:44.522573873Z",
     "start_time": "2024-01-25T01:43:44.427406625Z"
    }
   },
   "id": "839193d3d5558186",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "LLAMA_WEIGHTS_PATH = '/media/anique/Data/projects/llama-weights/llama2-7B'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:44.557686846Z",
     "start_time": "2024-01-25T01:43:44.453370520Z"
    }
   },
   "id": "84ccc81adda47d2d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:44.560325921Z",
     "start_time": "2024-01-25T01:43:44.496115264Z"
    }
   },
   "id": "3691087c1f97932e",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['wandb',\n 'evaluate.py',\n 'len_dist.png',\n 'scratch.py',\n 'huggingface',\n 'startpod',\n 'wikitable_train.json',\n 'lib',\n 'determine_max_length.py',\n 'requirements.txt',\n 'compute_accuracy.py',\n '.vscode',\n 'scripts',\n '.git',\n 'train.sh',\n 'parallama',\n '.gitignore',\n 'merged_dataset_insta_4chan.json',\n 'train.py',\n 'merge_lora.py',\n 'wikisql_lora',\n 'phase2_params',\n 'tests',\n 'mypy.ini',\n 'llama2-7B.pickle',\n 'gpt2',\n 'generate.py',\n 'alpaca_data_cleaned.json',\n '.idea',\n '.github',\n 'evaluation',\n 'memory.prof',\n 'README.md',\n 'hf_generate.py',\n 'hf_evaluate.py']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:44.595055166Z",
     "start_time": "2024-01-25T01:43:44.496224682Z"
    }
   },
   "id": "e2fd3bf78405a06d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the alpaca dataset\n",
    "with open('alpaca_data_cleaned.json', 'r') as f:\n",
    "    alpaca_data = json.load(f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:44.753007666Z",
     "start_time": "2024-01-25T01:43:44.496292632Z"
    }
   },
   "id": "a11fa08a5a8a4c7d",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_WEIGHTS_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:44.787671414Z",
     "start_time": "2024-01-25T01:43:44.615653419Z"
    }
   },
   "id": "a91778dff1388ccf",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41553/41553 [00:12<00:00, 3456.91it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = AlpacaDataset(path='alpaca_data_cleaned.json', split='train', tokenizer=tokenizer, split_percentage=0.8, alpaca_mix=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:57.026092866Z",
     "start_time": "2024-01-25T01:43:44.708035915Z"
    }
   },
   "id": "a8ddd79cf686a0ef",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a huggingface trainer compatible dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91b4d44c002e6470"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "list_dataset = []\n",
    "for sample in dataset:\n",
    "    list_dataset.append(\n",
    "        {\n",
    "            'prompt': sample[0], \n",
    "            'response': sample[1]\n",
    "        }\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:57.028723068Z",
     "start_time": "2024-01-25T01:43:57.012132376Z"
    }
   },
   "id": "cd49c2658e86646f",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# num_samples = len(list_dataset)\n",
    "# train_perc = 0.8\n",
    "# train_end_idx = int(train_perc * num_samples)\n",
    "# list_dataset_train = list_dataset[:train_end_idx]\n",
    "# list_dataset_val = list_dataset[train_end_idx:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:57.140081880Z",
     "start_time": "2024-01-25T01:43:57.012231466Z"
    }
   },
   "id": "645ea5cf2e333382",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hf_dataset = datasets.Dataset.from_list(list_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:57.190352579Z",
     "start_time": "2024-01-25T01:43:57.056105198Z"
    }
   },
   "id": "ffdedef55eea8235",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hf_dataset = hf_dataset.train_test_split(test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:57.269775687Z",
     "start_time": "2024-01-25T01:43:57.154757530Z"
    }
   },
   "id": "9de8e48c0ff28676",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'response'],\n        num_rows: 33179\n    })\n    test: Dataset({\n        features: ['prompt', 'response'],\n        num_rows: 8295\n    })\n})"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:57.273011669Z",
     "start_time": "2024-01-25T01:43:57.196188893Z"
    }
   },
   "id": "edd4195a6b777c29",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# function to tokenize the inputs and outputs \n",
    "def preprocess_function(examples, tokenizer=None, max_length=512):\n",
    "    batch_size = len(examples['prompt'])\n",
    "    model_inputs = tokenizer(examples['prompt'])\n",
    "    labels = tokenizer(examples['response'], add_special_tokens=False)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.eos_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    model_inputs['input_ids'] = torch.stack(model_inputs['input_ids'])\n",
    "    model_inputs['attention_mask'] = torch.stack(model_inputs['attention_mask'])\n",
    "    model_inputs['labels'] = torch.stack(model_inputs['labels'])\n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:43:57.299038099Z",
     "start_time": "2024-01-25T01:43:57.196334887Z"
    }
   },
   "id": "43973a978c9aca0",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/33179 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0271077112a6462e8f52e2cfba986058"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/8295 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "750e62f54aad41848f4c13a01626f647"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with accelerator.main_process_first():\n",
    "    hf_dataset_tk = hf_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer, 'max_length': 512}, remove_columns=hf_dataset['train'].column_names)\n",
    "accelerator.wait_for_everyone()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:10.562539081Z",
     "start_time": "2024-01-25T01:43:57.196431061Z"
    }
   },
   "id": "c6174b0b4b98b835",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['input_ids', 'attention_mask', 'labels'])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset_tk['train'][0].keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:12.762724304Z",
     "start_time": "2024-01-25T01:44:12.719724771Z"
    }
   },
   "id": "ddc61d7279e04836",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain the implications of deferred taxation\\n\\n### Response:\\n Deferred taxation is an accounting concept in which taxes are postponed to a future period, rather than paid during the current tax period. This can be done to take advantage of advantageous tax rules or to avoid taxation entirely. The implications of deferred taxation often rely on the entity's tax liability and the amount of the tax deferral. This concept can create a large advantage or disadvantage, depending on the type of business and tax rate.</s>\""
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(hf_dataset_tk['train'][0]['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:13.191038158Z",
     "start_time": "2024-01-25T01:44:13.138394386Z"
    }
   },
   "id": "a1b79e31c7b012c",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the huggingface trainer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9662950d280a827f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:16.507225058Z",
     "start_time": "2024-01-25T01:44:16.436951727Z"
    }
   },
   "id": "ef743dd6144885ff",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:17.945212405Z",
     "start_time": "2024-01-25T01:44:17.870548338Z"
    }
   },
   "id": "5cc6a12d86576402",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "EPOCHS = 7 \n",
    "BATCH_SIZE = 1\n",
    "n_accumulation_steps = 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:18.358999836Z",
     "start_time": "2024-01-25T01:44:18.274975481Z"
    }
   },
   "id": "32cb7139e3ce409d",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(hf_dataset_tk['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=default_data_collator)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:18.970930162Z",
     "start_time": "2024-01-25T01:44:18.863000693Z"
    }
   },
   "id": "c10c49abb32850b0",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n              2,     2,     2,     1, 13866,   338,   385, 15278,   393, 16612,\n            263,  3414, 29892,  3300,  2859,   411,   385,  1881,   393,  8128,\n           4340,  3030, 29889, 14350,   263,  2933,   393,  7128,  2486,  1614,\n           2167,   278,  2009, 29889,    13,    13,  2277, 29937,  2799,  4080,\n          29901,    13,  5328,   947,   278, 15572,   391,  4459,  7113,   278,\n           1095,   310,   278,  3143, 29973,    13,    13,  2277, 29937, 10567,\n          29901,    13, 29967, 10578,   756,  1063, 20042,   304,  1207,  4060,\n            310,   825,   670,  2834,   756,  4953, 29889,   940,   756,  5714,\n           1784,   310,   670,  2030,  7875, 29892,   322,   670,  5434,   338,\n          17999, 29889,    13,    13,  2277, 29937, 13291, 29901,    13,  2180,\n            278,  1095,   310,   278,  3143, 29892, 22838, 23880,  4966,  1319,\n            541,  1603, 24937,  1048,   278,  9815, 24496,   310,   670,  5434,\n          29889,   940,   756,  7232,   945,   287,  1735,   322,   756,  2041,\n            304,  2274,   278, 13500,   310,  5622,  5161,  2039,   297,  2834,\n          29889,     2]]),\n 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1]]),\n 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2180,\n            278,  1095,   310,   278,  3143, 29892, 22838, 23880,  4966,  1319,\n            541,  1603, 24937,  1048,   278,  9815, 24496,   310,   670,  5434,\n          29889,   940,   756,  7232,   945,   287,  1735,   322,   756,  2041,\n            304,  2274,   278, 13500,   310,  5622,  5161,  2039,   297,  2834,\n          29889,     2]])}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:20.688286860Z",
     "start_time": "2024-01-25T01:44:20.584641782Z"
    }
   },
   "id": "6e3604cce5e11d88",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d93f232c0c548c6afcaadc9ab07b047"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anique/anaconda3/envs/JAX_llama/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting model for PEFT training\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(LLAMA_WEIGHTS_PATH, torch_dtype=torch.bfloat16)\n",
    "print(\"Converting model for PEFT training\")\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:24.033434846Z",
     "start_time": "2024-01-25T01:44:21.524914967Z"
    }
   },
   "id": "1976405a8254d464",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.12433454005023165\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:27.949579057Z",
     "start_time": "2024-01-25T01:44:27.875663919Z"
    }
   },
   "id": "192de73de5adbc51",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_steps = len(train_dataloader) // n_accumulation_steps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:31.282603440Z",
     "start_time": "2024-01-25T01:44:31.264416696Z"
    }
   },
   "id": "75dc86eeb3d8c2cb",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=n_accumulation_steps, num_training_steps=n_steps * EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:32.829343672Z",
     "start_time": "2024-01-25T01:44:32.766888553Z"
    }
   },
   "id": "a5047a7f0db12889",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model, train_dataloader, optimizer, lr_scheduler = accelerator.prepare(model, train_dataloader, optimizer, lr_scheduler)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:37.294927198Z",
     "start_time": "2024-01-25T01:44:35.122089698Z"
    }
   },
   "id": "ca36023f41f80360",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Distributed environment: MULTI_GPU\nNum processes: 1\nProcess index: 0\nLocal process index: 0\nDevice: cuda:0\n\nMixed precision type: no"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.state"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:45.786710225Z",
     "start_time": "2024-01-25T01:44:45.649989340Z"
    }
   },
   "id": "bec6fae63cf7f412",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_ds_zero3 = False\n",
    "if getattr(accelerator.state, 'deepspeed_plugin', None):\n",
    "    is_ds_zero3 = accelerator.state.deepspeed_plugin.zero_stage == 3\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:44:50.234860979Z",
     "start_time": "2024-01-25T01:44:50.212617068Z"
    }
   },
   "id": "50e5499d7605915e",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anique/anaconda3/envs/JAX_llama/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 1086/33179 [06:34<3:17:07,  2.71it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    with TorchTracemalloc() as tracemalloc:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-25T01:44:51.299201150Z"
    }
   },
   "id": "62404ea9dc988807",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "a = next(iter(train_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-25T01:39:13.303503639Z"
    }
   },
   "id": "55d32b405fcd0202",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_args(**kwargs):\n",
    "    print(kwargs.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-25T01:39:13.305475892Z"
    }
   },
   "id": "c79b9c20a88f70d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_args(**a)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:39:13.311378806Z",
     "start_time": "2024-01-25T01:39:13.307110576Z"
    }
   },
   "id": "975384f675c1808f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:39:13.372611709Z",
     "start_time": "2024-01-25T01:39:13.332528812Z"
    }
   },
   "id": "b1e7857395a0a362",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "a['input_ids'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-25T01:39:13.332647747Z"
    }
   },
   "id": "7618506904fa1441",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(a['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-25T01:39:13.332709532Z"
    }
   },
   "id": "8854b71871f24ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[[2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29892,\n  3300,\n  2859,\n  411,\n  385,\n  1881,\n  393,\n  8128,\n  4340,\n  3030,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  1123,\n  29899,\n  3539,\n  278,\n  1494,\n  10541,\n  773,\n  1147,\n  5824,\n  297,\n  278,\n  4940,\n  260,\n  1947,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  10567,\n  29901,\n  13,\n  29902,\n  505,\n  18093,\n  263,\n  716,\n  1559,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  306,\n  18093,\n  263,\n  716,\n  1559,\n  29889,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  9984,\n  263,\n  1051,\n  310,\n  5320,\n  12785,\n  310,\n  10757,\n  304,\n  6356,\n  393,\n  5534,\n  1370,\n  4056,\n  338,\n  1855,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  22853,\n  12785,\n  310,\n  10757,\n  304,\n  6356,\n  393,\n  5534,\n  1370,\n  4056,\n  338,\n  1855,\n  3160,\n  29901,\n  13,\n  29896,\n  29889,\n  10231,\n  5534,\n  6238,\n  3698,\n  785,\n  6588,\n  10430,\n  6475,\n  1510,\n  263,\n  5534,\n  7910,\n  297,\n  10430,\n  310,\n  29871,\n  29896,\n  29889,\n  29955,\n  30073,\n  29943,\n  975,\n  278,\n  4940,\n  6462,\n  29871,\n  13,\n  29906,\n  29889,\n  9232,\n  1259,\n  16755,\n  14890,\n  322,\n  14751,\n  455,\n  414,\n  785,\n  278,\n  826,\n  20009,\n  21091,\n  338,\n  29871,\n  29896,\n  29945,\n  10151,\n  7968,\n  1135,\n  372,\n  471,\n  297,\n  278,\n  29871,\n  29896,\n  29929,\n  29947,\n  29900,\n  29879,\n  13,\n  29941,\n  29889,\n  20493,\n  7205,\n  11174,\n  785,\n  6475,\n  1510,\n  263,\n  14451,\n  310,\n  29871,\n  29947,\n  22831,\n  1951,\n  29871,\n  29896,\n  29947,\n  29955,\n  29900,\n  13,\n  29946,\n  29889,\n  22193,\n  2450,\n  310,\n  23474,\n  19922,\n  785,\n  11664,\n  17977,\n  683,\n  310,\n  22004,\n  20562,\n  29916,\n  680,\n  491,\n  288,\n  346,\n  550,\n  756,\n  8581,\n  22193,\n  2450,\n  29892,\n  6602,\n  292,\n  23585,\n  2894,\n  12903,\n  13,\n  29945,\n  29889,\n  3620,\n  297,\n  14826,\n  15038,\n  785,\n  18677,\n  14826,\n  4959,\n  526,\n  10231,\n  297,\n  10868,\n  322,\n  26171,\n  29889,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  1293,\n  472,\n  3203,\n  29871,\n  29941,\n  23633,\n  310,\n  321,\n  1218,\n  9045,\n  29891,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  12753,\n  23633,\n  310,\n  321,\n  1218,\n  9045,\n  29891,\n  526,\n  16710,\n  9128,\n  9045,\n  29892,\n  16710,\n  19119,\n  9045,\n  29892,\n  322,\n  263,\n  9263,\n  1463,\n  12045,\n  310,\n  17168,\n  293,\n  10267,\n  2129,\n  29889,\n  382,\n  1218,\n  9045,\n  29891,\n  508,\n  1371,\n  7344,\n  263,\n  9045,\n  29891,\n  7688,\n  322,\n  10032,\n  278,\n  12045,\n  310,\n  5192,\n  17135,\n  29892,\n  652,\n  370,\n  10778,\n  29892,\n  322,\n  3058,\n  4072,\n  310,\n  23900,\n  29889,\n  19814,\n  29892,\n  321,\n  1218,\n  9045,\n  29891,\n  508,\n  10032,\n  22884,\n  29892,\n  14505,\n  5864,\n  11174,\n  29892,\n  322,\n  7910,\n  3370,\n  322,\n  26702,\n  29889,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  5631,\n  403,\n  263,\n  1051,\n  310,\n  5320,\n  9687,\n  29879,\n  393,\n  1369,\n  411,\n  278,\n  5497,\n  1346,\n  29943,\n  30024,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  383,\n  1175,\n  16628,\n  29892,\n  12941,\n  2142,\n  295,\n  29892,\n  383,\n  542,\n  562,\n  1512,\n  29892,\n  5176,\n  285,\n  2722,\n  29892,\n  12030,\n  303,\n  7358,\n  29889,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29892,\n  3300,\n  2859,\n  411,\n  385,\n  1881,\n  393,\n  8128,\n  4340,\n  3030,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  29934,\n  10540,\n  445,\n  10541,\n  304,\n  11157,\n  967,\n  21503,\n  13,\n  13,\n  2277,\n  29937,\n  10567,\n  29901,\n  13,\n  855,\n  22155,\n  1549,\n  278,\n  25013,\n  508,\n  367,\n  2289,\n  26681,\n  292,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  5701,\n  8428,\n  292,\n  1549,\n  278,\n  25013,\n  508,\n  367,\n  5198,\n  575,\n  873,\n  1208,\n  4056,\n  29889,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  5631,\n  403,\n  1023,\n  25260,\n  393,\n  8453,\n  263,\n  2022,\n  29915,\n  29879,\n  9128,\n  5680,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  940,\n  750,\n  9307,\n  3277,\n  7254,\n  5076,\n  322,\n  281,\n  5301,\n  1999,\n  898,\n  11315,\n  29889,\n  3600,\n  5680,\n  892,\n  6035,\n  22999,\n  630,\n  491,\n  263,\n  4549,\n  432,\n  1450,\n  1220,\n  29892,\n  7970,\n  19595,\n  287,\n  491,\n  670,\n  15655,\n  29892,\n  2301,\n  16637,\n  4377,\n  29889,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  29907,\n  20440,\n  675,\n  278,\n  1494,\n  3838,\n  964,\n  302,\n  1309,\n  29879,\n  322,\n  1147,\n  5824,\n  29901,\n  22850,\n  29892,\n  12500,\n  29892,\n  1303,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  405,\n  1309,\n  29879,\n  29901,\n  22850,\n  29892,\n  7523,\n  29871,\n  13,\n  6565,\n  5824,\n  29901,\n  435,\n  3427,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29892,\n  3300,\n  2859,\n  411,\n  385,\n  1881,\n  393,\n  8128,\n  4340,\n  3030,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  12498,\n  278,\n  19039,\n  16549,\n  297,\n  445,\n  10541,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  10567,\n  29901,\n  13,\n  3868,\n  1754,\n  1854,\n  393,\n  278,\n  1842,\n  471,\n  16232,\n  29892,\n  322,\n  4340,\n  5514,\n  540,\n  7120,\n  372,\n  8951,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  940,\n  1754,\n  1854,\n  393,\n  278,\n  1842,\n  471,\n  16232,\n  322,\n  7120,\n  372,\n  8951,\n  29889,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  4002,\n  29581,\n  920,\n  304,\n  7344,\n  10992,\n  17182,\n  297,\n  263,\n  1559,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  512,\n  1797,\n  304,\n  7344,\n  10992,\n  17182,\n  297,\n  263,\n  1559,\n  29892,\n  366,\n  881,\n  1423,\n  372,\n  1432,\n  2846,\n  11405,\n  322,\n  2246,\n  372,\n  1283,\n  408,\n  4312,\n  29889,\n  887,\n  881,\n  884,\n  5146,\n  8570,\n  304,\n  278,\n  11029,\n  310,\n  278,\n  17182,\n  29892,\n  408,\n  372,\n  508,\n  4953,\n  640,\n  9103,\n  630,\n  470,\n  9391,\n  1623,\n  975,\n  931,\n  29889,\n  960,\n  278,\n  17182,\n  16410,\n  304,\n  1106,\n  7167,\n  3459,\n  470,\n  756,\n  263,\n  4319,\n  2413,\n  272,\n  29892,\n  372,\n  338,\n  931,\n  304,\n  505,\n  372,\n  3939,\n  29889,\n  19814,\n  29892,\n  372,\n  338,\n  4100,\n  304,\n  505,\n  596,\n  1559,\n  3348,\n  7612,\n  25704,\n  304,\n  1423,\n  363,\n  738,\n  4828,\n  322,\n  505,\n  278,\n  17182,\n  3939,\n  565,\n  5181,\n  29889,\n  2],\n [2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  1,\n  13866,\n  338,\n  385,\n  15278,\n  393,\n  16612,\n  263,\n  3414,\n  29889,\n  14350,\n  263,\n  2933,\n  393,\n  7128,\n  2486,\n  1614,\n  2167,\n  278,\n  2009,\n  29889,\n  13,\n  13,\n  2277,\n  29937,\n  2799,\n  4080,\n  29901,\n  13,\n  5618,\n  4234,\n  5279,\n  8640,\n  278,\n  1556,\n  20346,\n  1370,\n  2813,\n  29879,\n  29973,\n  13,\n  13,\n  2277,\n  29937,\n  13291,\n  29901,\n  13,\n  7579,\n  304,\n  278,\n  17920,\n  4623,\n  24819,\n  10550,\n  8907,\n  29892,\n  278,\n  3303,\n  3900,\n  5279,\n  756,\n  278,\n  1556,\n  20346,\n  1370,\n  2813,\n  29879,\n  411,\n  263,\n  3001,\n  310,\n  29871,\n  29945,\n  29892,\n  29947,\n  29900,\n  29900,\n  29889,\n  12710,\n  338,\n  3802,\n  5742,\n  411,\n  29871,\n  29945,\n  29892,\n  29953,\n  29900,\n  29900,\n  20346,\n  1370,\n  2813,\n  29879,\n  29889,\n  2]]"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset_tk['train'][:10]['input_ids']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T01:23:36.237836527Z",
     "start_time": "2024-01-25T01:23:36.140669345Z"
    }
   },
   "id": "ff58ef432df5eed2",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "36a0dcaefc3fcb95"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
